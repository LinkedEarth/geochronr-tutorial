[["index.html", "Time-uncertain data analysis in R Chapter 1 Welcome!", " Time-uncertain data analysis in R Nick McKay Last updated: 2022-02-07 Chapter 1 Welcome! Quantifying age uncertainties is a critical component of the paleosciences (paleoclimatology, paleoecology, paleontology), and one that has become commonplace in these fields this century. However, propagating these uncertainties through subsequent analysis and assessing their impact on the conclusions of a study remains rare. This book describes the theory and practice of time-uncertain data analysis in R, largely relying on the geoChronR package. geoChronR is an integrated framework that allows scientists to generate state-of-the-art age models for their records, create time-uncertain ensembles of their data, analyze those ensembles with a number of commonly-used techniques, and visualize their results in an intuitive way. A peer-reviewed paper describing geoChronR was published in March 2021 in Geochronology, check it out here This book, geoChronR, and other LinkedEarth tools are under steady development, and rely on input from and contributions from the community. If you have suggestions, requests, or would like to contribute to development, we’d love to get you more involved! "],["installation.html", "Chapter 2 Installing the software you need 2.1 Installing lipdR 2.2 Installing geoChronR", " Chapter 2 Installing the software you need You need R (&gt;=3.6) and we strongly recommend RStudio as an IDE. If you’re not sure which version of R you’re running, try R.version ## _ ## platform x86_64-apple-darwin17.0 ## arch x86_64 ## os darwin17.0 ## system x86_64, darwin17.0 ## status ## major 4 ## minor 1.0 ## year 2021 ## month 05 ## day 18 ## svn rev 80317 ## language R ## version.string R version 4.1.0 (2021-05-18) ## nickname Camp Pontanezen If that gives you an error, you likely have an older version, but this should work. sessionInfo() ## R version 4.1.0 (2021-05-18) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 11.4 ## ## Matrix products: default ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bookdown_0.22 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-152 spatstat.sparse_2.0-0 bitops_1.0-7 matrixStats_0.59.0 doParallel_1.0.16 ## [6] RColorBrewer_1.1-2 httr_1.4.2 tools_4.1.0 backports_1.2.1 bslib_0.2.5.1 ## [11] utf8_1.2.1 R6_2.5.0 rpart_4.1-15 mgcv_1.8-35 DBI_1.1.1 ## [16] colorspace_2.0-2 sp_1.4-5 tidyselect_1.1.1 gridExtra_2.3 compiler_4.1.0 ## [21] Bchron_4.7.6 sass_0.4.0 scales_1.1.1 checkmate_2.0.0 spatstat.data_2.1-0 ## [26] ggridges_0.5.3 pbapply_1.4-3 goftest_1.2-2 stringr_1.4.0 digest_0.6.27 ## [31] spatstat.utils_2.2-0 rmarkdown_2.9 R.utils_2.10.1 jpeg_0.1-8.1 pkgconfig_2.0.3 ## [36] htmltools_0.5.1.1 maps_3.3.0 rlang_0.4.11 ggthemes_4.2.4 rstudioapi_0.13 ## [41] rbacon_2.5.6 jquerylib_0.1.4 farver_2.1.0 generics_0.1.0 lomb_2.0 ## [46] jsonlite_1.7.2 dplyr_1.0.7 R.oo_1.24.0 magrittr_2.0.1 dotCall64_1.0-1 ## [51] Matrix_1.4-0 Rcpp_1.0.7 munsell_0.5.0 fansi_0.5.0 abind_1.4-5 ## [56] viridis_0.6.1 lifecycle_1.0.0 R.methodsS3_1.8.1 stringi_1.6.2 yaml_2.2.1 ## [61] MASS_7.3-54 plyr_1.8.6 grid_4.1.0 parallel_4.1.0 crayon_1.4.1 ## [66] deldir_0.2-10 astrochron_1.0 lattice_0.20-44 geoChronR_1.1.0 splines_4.1.0 ## [71] tensor_1.5 mapproj_1.2.7 knitr_1.33 pillar_1.6.1 multitaper_1.0-15 ## [76] spatstat.geom_2.2-2 codetools_0.2-18 reshape2_1.4.4 XML_3.99-0.6 glue_1.4.2 ## [81] evaluate_0.14 BiocManager_1.30.16 foreach_1.5.1 png_0.1-7 vctrs_0.3.8 ## [86] spam_2.7-0 nuspectral_1.0 tweenr_1.0.2 RgoogleMaps_1.4.5.3 spatstat.core_2.2-0 ## [91] gtable_0.3.0 purrr_0.3.4 polyclip_1.10-0 tidyr_1.1.3 assertthat_0.2.1 ## [96] ggplot2_3.3.5 xfun_0.24 ggforce_0.3.3 coda_0.19-4 oxcAAR_1.1.1 ## [101] IDPmisc_1.1.20 viridisLite_0.4.0 signal_0.7-7 tibble_3.1.2 dplR_1.7.2 ## [106] iterators_1.0.13 IntCal_0.2.0 fields_12.5 ggmap_3.0.0.903 rEDM_1.9.0 ## [111] ellipsis_0.3.2 2.1 Installing lipdR The lipdR package is your gateway to working with LiPD data. To install lipdR, you’ll need the remotes package if you don’t already have it. install.packages(&quot;remotes&quot;) ## ## The downloaded binary packages are in ## /var/folders/09/hb50c49x3njd333vkfyk375m0000gn/T//RtmpAxxZ9t/downloaded_packages remotes::install_github(&quot;nickmckay/lipdR&quot;) ## Skipping install of &#39;lipdR&#39; from a github remote, the SHA1 (c4b5b063) has not changed since last install. ## Use `force = TRUE` to force installation 2.2 Installing geoChronR Finally, you can install geoChronR using a similar command: remotes::install_github(&quot;nickmckay/GeoChronR&quot;) This should also install all the dependencies you need. geoChronR is built on top of a large collection of other packages, which means it can take a while to install, but also it means that changes to those packages sometimes causes errors in geoChronR. If you get an error during installation, please report it here, and we’ll fix it as soon as possible. "],["data.html", "Chapter 3 Data in geoChronR 3.1 Loading a LiPD file 3.2 Loading multiple LiPD datasets", " Chapter 3 Data in geoChronR geoChronR was designed for data structured in the LiPD format. Actually, much of the LiPD framework was developed to accommodate the needs of geoChronR, so by far, you’ll find it easiest to work with data in the structured, metadata-rich LiPD format. All of the examples and exercises in this book will use LiPD datasets. If you have data you want to analyze in geoChronR, the best and easiest way to get those data into the LiPD format is at the LiPD playground. It takes some time to learn the structure, and some time to annotate your data, but it will save you time down the road. If you really don’t want to lipdify your data, we are working on ways to take advantage of geoChronR tools with simpler data structures. This will mean more work in geoChronR, since you’ll have to specify a lot information that would normally be included in a LiPD file, but there are times when this makes sense. Once this functionality is operable, we’ll add a module demonstrating how this works. 3.1 Loading a LiPD file Now you can load the lipdR library, and load some data. library(lipdR) ## Registered S3 method overwritten by &#39;cli&#39;: ## method from ## print.boxx spatstat.geom library(geoChronR) ## ## Attaching package: &#39;geoChronR&#39; ## The following objects are masked from &#39;package:lipdR&#39;: ## ## createTSid, pullTsVariable L &lt;- readLipd(&quot;https://lipdverse.org/Temp12k/current_version/GEOFAR_KF16_MgCa.Repschlager.2016.lpd&quot;) ## [1] &quot;reading: GEOFAR_KF16_MgCa.Repschlager.2016.lpd&quot; You can load a LiPD file from the web, as shown above, or locally on your computer. If you leave the input blank (e.g. L &lt;- readLipd()), you can interactively choose a file on your computer. Exercise 3.1 Explore the Temp12k collection on lipdverse.org, and load a file into R using lipdR with two different approaches: a) directly using the url as above, and b) download the file to your computer, and load it interactively. Click for Answer to 3.1a We’ll use this dataset. First we’ll get it directly from the web like this: lipd &lt;- readLipd(&quot;https://lipdverse.org/Temp12k/current_version/HeshangCave.Wang.2018.lpd&quot;) ## [1] &quot;reading: HeshangCave.Wang.2018.lpd&quot; note that the extension is “.lpd” not “.html” Click for Answer to 3.1b This time, just go to the dataset and download the data to your computer, then run lipd &lt;- readLipd() and select the file interactively. Once you have a LiPD object loaded in R, there are a few things you can do. First, if you want a quick and dirty visualization of what’s inside the dataset, use plotLipd(). If you know what the variables are you want to plot beforehand, you can specify them like this: summary_plot &lt;- plotSummary(L,paleo.data.var = &quot;temperature&quot;,chron.age.var = &quot;age&quot;,chron.depth.var = &quot;depthMid&quot;,chron.age.14c.var = NULL) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; print(summary_plot) ## TableGrob (4 x 4) &quot;arrange&quot;: 4 grobs ## z cells name grob ## 1 1 (1-1,1-2) arrange gTree[GRID.gTree.11] ## 2 2 (1-2,3-4) arrange gtable[layout] ## 3 3 (2-4,1-2) arrange gtable[layout] ## 4 4 (3-4,3-4) arrange gtable[layout] But most of the time, you’ll just run it in interactive mode and select what you want. plotSummary(lipd) One of the components of plotSummary() is a site map, which is created with the function mapLipd(). Exercise 3.2 Check out the documentation for mapLipd() (type ?mapLipd), and tinker with the options to produce different types of maps. 3.1.1 Extract a variable from a LiPD object In geoChronR, it is often useful to extract one or two variables from a LiPD object and then use them for subsequent analysis and visualization. The key function for this is selectData(). Typically, you’ll use it interactive mode to find the variable your looking for, either in the paleoData or chronData sections of the LiPD file. #grab the calibrated temperature record temp &lt;- selectData(L,paleo.or.chron = &quot;paleoData&quot;) #grab the original dates from the chronData c14 &lt;- selectData(L,paleo.or.chron = &quot;chronData&quot;) If there’s a lot of variables in the file, and you kinda know what you’re looking for, you can add a var.name guess and it will narrow it down to options that seem likely, or select one automatically if it’s close. Exercise 3.2 Enter “d18o” as the var.name in selectData() and see how it narrows down your options. Of course, you can also specify all of the options in selectData(), and then get exactly what you want non interactively. This includes data from model ensembleTables or summaryTables, in addition to the measurementTables you use more frequently. For example: mgCa &lt;- selectData(L,var.name = &quot;mg/ca&quot;,paleo.or.chron = &quot;paleoData&quot;,table.type = &quot;meas&quot;,meas.table.num = 1) ## [1] &quot;Found it! Moving on...&quot; 3.2 Loading multiple LiPD datasets In addition to loading single files, readLipd() can also load a whole directory, or a url that points to a zip file full of files. eur &lt;- readLipd(&quot;https://lipdverse.org/geoChronR-examples/euro/Euro2k.zip&quot;) ## [1] &quot;reading: Arc-AkademiiNaukIceCap.Opel.2013.lpd&quot; ## [1] &quot;reading: Arc-Forfjorddalen.McCarroll.2013.lpd&quot; ## [1] &quot;reading: Arc-GulfofAlaska.Wilson.2014.lpd&quot; ## [1] &quot;reading: Arc-Indigirka.Hughes.1999.lpd&quot; ## [1] &quot;reading: Arc-Jamtland.Wilson.2016.lpd&quot; ## [1] &quot;reading: Arc-Kittelfjall.Bjorklund.2012.lpd&quot; ## [1] &quot;reading: Arc-PolarUrals.Wilson.2015.lpd&quot; ## [1] &quot;reading: Arc-Tjeggelvas.Bjorklund.2012.lpd&quot; ## [1] &quot;reading: Arc-Tornetrask.Melvin.2012.lpd&quot; ## [1] &quot;reading: Eur-CentralandEasternPyrenees.Pla.2004.lpd&quot; ## [1] &quot;reading: Eur-CentralEurope.Dobrovoln.2009.lpd&quot; ## [1] &quot;reading: Eur-CoastofPortugal.Abrantes.2011.lpd&quot; ## [1] &quot;reading: Eur-EasternCarpathianMountains.Popa.2008.lpd&quot; ## [1] &quot;reading: Eur-EuropeanAlps.Bntgen.2011.lpd&quot; ## [1] &quot;reading: Eur-FinnishLakelands.Helama.2014.lpd&quot; ## [1] &quot;reading: Eur-LakeSilvaplana.Larocque-Tobler.2010.lpd&quot; ## [1] &quot;reading: Eur-LakeSilvaplana.Trachsel.2010.lpd&quot; ## [1] &quot;reading: Eur-Ltschental.Bntgen.2006.lpd&quot; ## [1] &quot;reading: Eur-MaritimeFrenchAlps.Bntgen.2012.lpd&quot; ## [1] &quot;reading: Eur-NorthernScandinavia.Esper.2012.lpd&quot; ## [1] &quot;reading: Eur-NorthernSpain.Martn-Chivelet.2011.lpd&quot; ## [1] &quot;reading: Eur-RAPiD-17-5P.Moffa-Sanchez.2014.lpd&quot; ## [1] &quot;reading: Eur-Seebergsee.Larocque-Tobler.2012.lpd&quot; ## [1] &quot;reading: Eur-SpanishPyrenees.Dorado-Linan.2012.lpd&quot; ## [1] &quot;reading: Eur-SpannagelCave.Mangini.2005.lpd&quot; ## [1] &quot;reading: Eur-Stockholm.Leijonhufvud.2009.lpd&quot; ## [1] &quot;reading: Eur-Tallinn.Tarand.2001.lpd&quot; ## [1] &quot;reading: Eur-TatraMountains.Bntgen.2013.lpd&quot; ## [1] &quot;reading: Ocn-AqabaJordanAQ18.Heiss.1999.lpd&quot; ## [1] &quot;reading: Ocn-AqabaJordanAQ19.Heiss.1999.lpd&quot; ## [1] &quot;reading: Ocn-RedSea.Felis.2000.lpd&quot; If you load multiple files, it will create an object called a “multi-lipd,” which is just a list of lipd objects. And we can use some of the same functions. For example, let’s make a map of our multi-lipd object: mapLipd(eur,projection = &quot;mollweide&quot;,global = TRUE) These multi-lipd objects are convenient for getting a lot of data into R, but trying to work with the data inside can get pretty tricky pretty fast. When working with multiple datasets, you’ll almost always want to create a timeseries (TS) object, or formally a “lipd-ts” object. You can do that using the ‘extractTs()’ function or ’as.lipdTs()` from the lipdR package. TS &lt;- extractTs(eur) TS &lt;- as.lipdTs(eur) The LiPD-TS object is a “flattened” version of the dataset, it’s much less hierarchical and each entry corresponds to a column in a table. By default, extractTs() will get all the variables in measurementTables in paleoData objects, but you can also get variables from other tables and from the chronData objects by changing the options. See ?extractTs for details. Now we can use the TS version of plotSummary to summarize the whole collection of data. summ &lt;- plotSummaryTs(TS,age.var = &quot;year&quot;) Let’s take a deeper look at the options: Exercise 3.3 Explore the options in plotSummary. Create a new version that colors the dots and availability plot by PAGES 2k Region (the variable name is geo_pages2kRegion) Although LiPD-TS objects are more convenient to work with, they’re still list-based and difficult to explore visually. If you’re used to working in the tidyverse framework, you’ll likely find it useful to convert the data into a tibble (a type of data.frame). In lipdR, this is a “lipd-ts-tibble” object, but it’s also just a tibble, so it unlocks many, many options in RStudio and tidyverse. To do so, use the ts2tibble() or as.lipdTsTibble() functions to create tibble. ts.tib &lt;- ts2tibble(TS) If you’re in RStudio, you can now explore the contents of the object grapically. This produces a large tibble, so it’s still a little unwieldy. Importantly, some of the columns are nested (depth, age, year, paleoData_values), so all of the timeseries data are included in each row, for each timeseries. You can learn more about nested tibbles here. As the tidyverse has grown, we’ve increasingly moved our lipdR and geoChronR workflows into this model. Take a look at how this unlocks some pretty efficient workflows. 3.2.1 Filtering It’s often the case that the a collection of LiPD files includes far more data than we actually need. Let’s use dplyr to get just the data we need. Let’s say that we only want variables that are European tree ring data that were used in Pages2k. You can explore the variables in the data.frame, and the values in each field, and then compose an expression: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union filtered.tib &lt;- ts.tib %&gt;% filter(between(geo_latitude,30,80) &amp; between(geo_longitude,-30,60)) %&gt;% #restrict the coordinates to just Europe filter(archiveType == &quot;tree&quot;) %&gt;% filter(paleoData_useInGlobalTemperatureAnalysis == TRUE) Let’s take a look at the result! nTS &lt;- as.lipdTs(filtered.tib) #convert it back (for now) plotSummaryTs(nTS,age.var = &quot;year&quot;,f = .1) ## TableGrob (5 x 3) &quot;arrange&quot;: 2 grobs ## z cells name grob ## 1 1 (1-3,1-3) arrange gtable[layout] ## 2 2 (4-5,1-3) arrange gtable[layout] Great, it looks like our filtering worked! It looks like most of the data go back about 1000 years, but a few records are much longer. Next, we’d like to restrict our analysis to just 1000 - 2000 AD. This is possible in a nested tibble, but it’s often useful to have a longer version, one where every row in the data.frame corresponds to a single year-value pair, rather than a whole timeseries. This is the purpose of lipd-ts-tibble-long objects, the final lipd object type. This is useful if you want easy access to all the data in one large tibble. We can convert our data to this structure using tidyTs() or, you guessed it, as.lipdTsTibbleLong() longTib &lt;- as.lipdTsTibbleLong(filtered.tib) %&gt;% dplyr::filter(between(year,1000,2000)) ## creating a lipdTsTibbleLong using year as the age.var Take a look at our new tibble, we restricted the time range, but it still has 11,350 rows! As you can imagine, loading in a few hundred LiPD datasets and converting them into lipd-ts-tibble-long objects will result in tibbles of several million rows, which might cause your computer problems! Let’s take a quick look at these filtered data. The plotTimeseriesStack() function lets you make a quick version of the classic paleoclimate stack plots. Check out this tutorial for a deep dive. Here we’ll just plot our data, and color it by the proxy type. plotTimeseriesStack(longTib,color.var = &quot;paleoData_proxy&quot;) And there it is! Not too bad, but I don’t love that default colorscheme with only 3 values. Exercise 3.4 Explore the options in plotTimeseriesStack. Create a new version that: Has better colors! Changes the line thickness. What does invert.var do? Replot this, but only showing the MXD data. "],["agemodelling.html", "Chapter 4 Age modelling in geoChronR 4.1 Bacon 4.2 BChron 4.3 OxCal 4.4 Banded Age Modelling 4.5 Chapter project.", " Chapter 4 Age modelling in geoChronR geoChronR quantifies the uncertainties due to time uncertainty by taking advantage of ensembles of plausible age histories for one or more datasets. This means that often an early step in the geoChronR workflow is generating age ensembles. Most modern age modelling approaches quantify uncertainties using methods that rely on ensembles, however preserving, extracting, and storing those uncertainties for subsequent analysis can be challenging. geoChronR helps with this! In this chapter, we’ll go through the workflow of generating age models with four methods that are integrated into geoChronR library(lipdR) library(geoChronR) library(ggplot2) library(magrittr) tana &lt;- readLipd(&quot;https://lipdverse.org/Temp12k/1_0_2/TanaLake.Loomis.2015.lpd&quot;) ## [1] &quot;reading: TanaLake.Loomis.2015.lpd&quot; 4.1 Bacon The Bayesian ACcumulatiON (Bacon) algorithm (M. Blaauw and Christen 2011) is one of the most broadly used age-modelling techniques, and was designed to take advantage of prior knowledge about the distribution and autocorrelation structure of sedimentation rates in a sequence to better quantify uncertainty between dated levels. Bacon divides a sediment sequence into a parameterized number of equally-thick segments; most models use dozens to hundreds of these segments. Bacon then models sediment deposition, with uniform accumulation within each segment, as an autoregressive gamma process, where both the amount of autocorrelation and the shape of the gamma distribution are given prior estimates. The algorithm employs an adaptive Markov Chain Monte Carlo algorithm that allows for Bayesian learning to update these variables given the age-depth constraints, and converge on a distribution of age estimates for each segment in the model. Bacon has two key parameters: the shape of the accumulation prior, and the segment length, which can interact in complicated ways (Trachsel and Telford 2017). In our experience, the segment length parameter has the greatest impact on the ultimate shape and amount of uncertainty simulated by Bacon, as larger segments result in increased flexibility of the age-depth curve, and increased uncertainty between dated levels. Bacon is written in C++ and R, with an R interface. More recently, the authors released an R package “rbacon” (Maarten Blaauw, Christen, and Aquino L. 2020), which geoChronR leverages to provide access to the algorithm. Bacon will optionally return a thinned subset of the stabilized MCMC accumulation rate ensemble members, which geoChronR uses to form age ensemble members for subsequent analysis. tana &lt;- runBacon(tana, lab.id.var = &#39;LabID&#39;, age.14c.var = &#39;age14C&#39;, age.14c.uncertainty.var = &#39;age14CUnc&#39;, age.var = &#39;age&#39;, age.uncertainty.var = &#39;1SD&#39;, depth.var = &#39;depth&#39;, reservoir.age.14c.var = NULL, reservoir.age.14c.uncertainty.var = NULL, rejected.ages.var = NULL, accept.suggestions = TRUE) Great! If all went well Bacon ran, and geoChronR grabbed the ensembles for future use. What kind of future use? Well, let’s start with plotting. The plotChronEns() function is great for making, quick, but pretty nice, figures to show an age model ensemble. It has a lot of options for customization (check out ?plotChronEns). Lastly, what it returns is a ggplot2 object, meaning that you can further customize it! Let’s see how it goes! plotChronEns(tana) + ggtitle(&quot;Tana Lake - default Bacon model&quot;) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;plotting your chron ensemble. This make take a few seconds...&quot; ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. That was easy! But you’ll have to explore the options to fully customize your figure. Exercise 4.1 Explore the parameter choices in plotChronEns. Can you a) change the confidence interval colors and b) quantiles? c) Change the type of distribution plotted for the dates d) and their color and transparency? e) what does truncate.dist do? 4.2 BChron BChron (Haslett and Parnell 2008; A. Parnell et al. 2008) uses a similar approach, using a continuous Markov monotone stochastic process coupled to a piecewise linear deposition model. This simplicity allows semi-analytical solutions that make BChron computationally efficient. BChron was originally intended to model radiocarbon-based age-depth models in lake sedimentary cores of primarily Holocene age, but its design allows broader applications. In particular, modeling accumulation as additive independent gamma increments is appealing for the representation of hiatuses, particularly for speleothem records, where accumulation rate can vary quite abruptly between quiescent intervals of near-constant accumulation (A. C. Parnell, Buck, and Doan 2011; Dee et al. 2015; Hu, Emile-Geay, and Partin 2017). The downside of this assumption is that BChron is known to exaggerate age uncertainties in cases where sedimentation varies smoothly (Trachsel and Telford 2017). Bchron has several key parameters, which allow a user to encode their specific knowledge about their data. In particular, the outlierProbs parameter is useful in giving less weight to chronological tie points that may be considered outliers, either because they create a reversal in the stratigraphic sequence, or because they were flagged during analysis (e.g. contamination). This is extremely useful for radiocarbon-based chronologies where the top age may not be accurately measured for modern samples. The thetaMhSd, psiMhSd, and muMhSd parameters control the Metropolis-Hastings standard deviation for the age parameters and Compound Poisson-Gamma scale and mean respectively, which influence the width of the ensemble between age control tie points. geoChronR uses the same default values as the official Bchron package, and we recommend that users only change them if they have good cause for doing so. tana &lt;- runBchron(tana, iter = 10000, model.num = 2, lab.id.var = &#39;LabID&#39;, age.14c.var = &#39;age14C&#39;, age.14c.uncertainty.var = &#39;age14CUnc&#39;, age.var = &#39;age&#39;, age.uncertainty.var = &#39;1SD&#39;, depth.var = &#39;depth&#39;, reservoir.age.14c.var = NULL, reservoir.age.14c.uncertainty.var = NULL, rejected.ages.var = NULL) plotChronEns(tana,model.num = 2,truncate.dist = .0001) + ggtitle(&quot;Tana Lake - default Bchron model&quot;) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;plotting your chron ensemble. This make take a few seconds...&quot; ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. 4.3 OxCal The OxCal software package has a long history and extensive tools for the statistical treatment of radiocarbon and other geochronological data (Bronk Ramsey 1995). In Bronk Ramsey (2008), age-depth modelling was introduced with three options for modelling depositional processes that are typically useful for sedimentary sequences: uniform, varve, and Poisson deposition models, labeled U-sequence, V-sequence and P-sequence, respectively. The Poisson-based model is the most broadly applicable for sedimentary, or other accumulation-based archives (e.g. speleothems), and although any sequence type can be used in geoChronR, most users should use a P-sequence, which is the default. Analogously to segment length parameter in Bacon, the k parameter (called eventsPerUnitLength in geoChronR), controls how many events are simulated per unit of depth, and has a strong impact on the flexibility of the model, as well as the amplitude of the resulting uncertainty. As the number of events increases, the flexibility of the model, and the uncertainties, decrease. Trachsel and Telford (2017) found that this parameter has a large impact on the accuracy of the model, more so than the choices made in Bacon or Bchron. Fortunately, Bronk Ramsey et al. (2010) made it possible for k to be treated as a variable, and the model will estimate the most likely values of k given a prior estimate and the data. The downside of this flexibility is that this calculation can greatly increase the convergence time of the model. Oxcal is written in C++, with an interface in R (Martin et al. 2018). Oxcal does not typically calculate posterior ensembles for a depth sequence, but can optionally output MCMC posteriors at specified levels in the sequence. geoChronR uses this feature to extract ensemble members for subsequent analysis. tana &lt;- runOxcal(tana,model.num = 3, lab.id.var = &#39;LabID&#39;, age.14c.var = &#39;age14C&#39;, age.14c.uncertainty.var = &#39;age14CUnc&#39;, age.var = &#39;age&#39;, age.uncertainty.var = &#39;1SD&#39;, depth.var = &#39;depth&#39;, reservoir.age.14c.var = NULL, reservoir.age.14c.uncertainty.var = NULL, rejected.ages.var = NULL, events.per.unit.length = .05, depth.interval = 20) ## Oxcal is installed but Oxcal executable path is wrong. Let&#39;s have a look... ## Oxcal path set! ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## Looking for age uncertainty ## No variable called sd, or choosing is enforced (always.choose = TRUE) ## Looking for laboratory ID ## [1] &quot;Found it! Moving on...&quot; ## Looking for radiocarbon ages ## [1] &quot;Found it! Moving on...&quot; ## Looking for 1-sigma radiocarbon age uncertainty (+/-) ## [1] &quot;Found it! Moving on...&quot; ## Looking for calibrated/calendar ages ## [1] &quot;Found it! Moving on...&quot; ## Looking for 2-sigma calibrated age uncertainty (+/-) ## [1] &quot;Found it! Moving on...&quot; ## Looking for depth or position ## [1] &quot;Found it! Moving on...&quot; ## Looking for radiocarbon reservoir age offsets (deltaR) ## radiocarbon reservoir age offsets (deltaR) does not seem to exist, moving on. ## Looking for radiocarbon reservoir age offsets (deltaR) uncertainties ## radiocarbon reservoir age offsets (deltaR) uncertainties does not seem to exist, moving on. ## Looking for rejected ages ## rejected ages does not seem to exist, moving on. ## [1] &quot;Variable choices for reuse...&quot; ## For future reference: here are the options you chose: ## Find later with getLastVarString() ## lab.id.var = &#39;LabID&#39;, age.14c.var = &#39;age14C&#39;, age.14c.uncertainty.var = &#39;age14CUnc&#39;, age.var = &#39;age&#39;, age.uncertainty.var = &#39;1SD&#39;, depth.var = &#39;depth&#39;, reservoir.age.14c.var = NULL, reservoir.age.14c.uncertainty.var = NULL, rejected.ages.var = NULL, ## Oxcal is now running, depending on your settings and your computer, this may take a few minutes to several hours. The model is complete when a table of model diagnostics appears. plotChronEns(tana,model.num = 3,truncate.dist = .0001) + ggtitle(&quot;Tana Lake - Oxcal model&quot;) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;plotting your chron ensemble. This make take a few seconds...&quot; ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. 4.3.1 Let’s compare these models. First, lets use selectData() to pull the depth and ageEnsemble variables for each model. The selectData() function is introduced in section . ensBacon &lt;- selectData(tana, var.name = &quot;ageEnsemble&quot;, paleo.or.chron = &quot;chronData&quot;, model.num = 1, table.type = &quot;ensemble&quot;) depthBacon &lt;- selectData(tana, var.name = &quot;depth&quot;, paleo.or.chron = &quot;chronData&quot;, model.num = 1, table.type = &quot;ensemble&quot;) ensBchron &lt;- selectData(tana, var.name = &quot;ageEnsemble&quot;, paleo.or.chron = &quot;chronData&quot;, model.num = 2, table.type = &quot;ensemble&quot;) depthBchron &lt;- selectData(tana, var.name = &quot;depth&quot;, paleo.or.chron = &quot;chronData&quot;, model.num = 2, table.type = &quot;ensemble&quot;) ensOxcal &lt;- selectData(tana, var.name = &quot;ageEnsemble&quot;, paleo.or.chron = &quot;chronData&quot;, model.num = 3, table.type = &quot;ensemble&quot;) depthOxcal &lt;- selectData(tana, var.name = &quot;depth&quot;, paleo.or.chron = &quot;chronData&quot;, model.num = 3, table.type = &quot;ensemble&quot;) Now that we have all the data extracted, we can use the plotTimeseriesEnsRibbons() function to plot each of the modeled age-depth relationships and their uncertainties. We will use the magrittr “pipe” function %&gt;% to pass the output of one plot into the next to build up a complex figure. We’ll also use different colors and transparencies so we can distinguish the different models. plotTimeseriesEnsRibbons(X = ensBacon,Y = depthBacon) %&gt;% plotTimeseriesEnsRibbons(X = ensBchron,Y = depthBchron, alp = .7, color.high = &quot;DarkGreen&quot;, color.line = &quot;Green&quot;) %&gt;% plotTimeseriesEnsRibbons(X = ensOxcal,Y = depthOxcal, alp = .7, color.high = &quot;DarkBlue&quot;, color.line = &quot;Blue&quot;) %&gt;% plotModelDistributions(tana,add.to.plot = .) + #here we use the ggplot + scale_y_reverse() All geoChronR plotting functions return ggplot2 objects, so we can modify the scale by adding a layer using + using the ggplot2 model. Exercise 4.2 Where do the models agree? Where do they differ? Do you think one is better than the others? After you’ve answered, click for next step The OxCal model is considerably more flexible than the Bacon model, which leaves outliers off the main trend. If you wanted to make the OxCal model less flexible, which parameter(s) would you change? Alternatively, if you wanted to make the Bacon model more flexible, which parameter(s) would you change in the Bacon model? Try making a change to parameters in either Bacon or OxCal to make the models more similar (note, Bacon runs much faster, so I’d probably try that one first) Finally, how should you decide whether a more or less flexible model is better? 4.3.2 Creating a multimodel ensemble Sometimes, there are good reason to believe that because of it’s design, or underlying assumptions, one model may be superior to the others, in which case you should choose that model. However, frequently, it’s unclear which model to choose, or to objectively pick on model over another. In this case, you might want to create a multimodel ensemble that incorporates model structural uncertainty into your uncertainty structure. This is pretty straightforward in geoChronR. Here, we’ll create a fourth model that combines these three into a “Grand Ensemble” using createMultiModelEnsemble tana &lt;- createMultiModelEnsemble(tana, models.to.combine = 1:3, depth.interval =10, n.ens = 1000) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; Exercise 4.3 Use plotChronEns() and plotModelDistributions() to visualize your multi-model age model. Hint #1 First plot the chronEns, then use the “add.to.plot” parameter to add in the distributions. Hint #2 Something with this structure is what you’re looking for plotChronEns() %&gt;% plotModelDistributions() Exercise 4.4 Now that you’ve got plotting working, try changing the choices made in createMultiModelEnsemble. Specifically, what is the impact of changing, depth.interval, n.ens, or depth.sequence? Use the documentation for help! Exercise 4.5 Add your final multi model ensemble to the figure that showed the three original age models above. Does it look like a combination of the three? 4.3.3 Mapping the age ensemble to the paleoData measurements Great, our LiPD file now has an age ensemble (actually 4 age ensembles!) that we can use in subsequent analysis. We could write out our LiPD file right now using lipdR::writeLipd(tana), for future work, or share with a colleague, and when we load it back in, all of our ensembles will be there, ready to go! But for now, let’s think about the next step in our analysis. We want to look at our paleoenvironmental data in the context of the age uncertainties. So let’s take a look at the paleoData! #First, create a tibble from the paleoata paleo &lt;- extractTs(tana) %&gt;% ts2tibble() #Now you can explore that much more easily - here are all the variable names in all the measurementTables in the paleoData. paleo$paleoData_variableName ## [1] &quot;core&quot; &quot;year&quot; &quot;age&quot; &quot;Section&quot; &quot;Composite_depth&quot; &quot;temperature&quot; &quot;error&quot; ## [8] &quot;depth&quot; &quot;Ti&quot; &quot;dDwax&quot; &quot;dDwax Corrected&quot; &quot;dD error&quot; &quot;d13Cwax&quot; &quot;d13C Error&quot; ## [15] &quot;age&quot; &quot;depth&quot; It looks like depth in this dataset is “Composite_depth,” and the median age vector is here, but the age ensemble is not! Why not? Well, the ensemble chronology in a model may or may not have values corresponding to the paleoclimatic or paleoenvironmental measurements in paleoData. Each of our models have different depth scales, and they’re all different than our paleoclimate data. So we need to “map” the model ensemble values to a measurement table in paleoData, so we can estimate the age uncertainty on each value. To do this we use the mapAgeEnsembleToPaleoData() function. tana &lt;- mapAgeEnsembleToPaleoData(tana, age.var = &quot;ageEnsemble&quot;, model.num = 4, paleo.depth.var = &quot;Composite_depth&quot;, paleo.meas.table.num = 1) Now let’s look at the paleoData again: paleo &lt;- extractTs(tana) %&gt;% ts2tibble() paleo$paleoData_variableName ## [1] &quot;core&quot; &quot;year&quot; &quot;age&quot; &quot;Section&quot; &quot;Composite_depth&quot; &quot;temperature&quot; &quot;error&quot; ## [8] &quot;ageEnsemble&quot; &quot;depth&quot; &quot;Ti&quot; &quot;dDwax&quot; &quot;dDwax Corrected&quot; &quot;dD error&quot; &quot;d13Cwax&quot; ## [15] &quot;d13C Error&quot; &quot;age&quot; &quot;depth&quot; Great, now we have an ageEnsemble variable in our paleoData (and our tibble!) 4.3.4 Creating a timeseries plot as a spaghetti plot of lines Let’s visualize the reconstructed temperature with age uncertainties. First, we’ll use selectData() again to get our mapped ensemble and temperature data: tana.ae &lt;- selectData(tana,var.name = &quot;ageEnsemble&quot;,meas.table.num = 1) ## [1] &quot;Found it! Moving on...&quot; tana.temp &lt;- selectData(tana,var.name = &quot;temperature&quot;,meas.table.num = 1) ## [1] &quot;Found it! Moving on...&quot; OK, we’re ready to plot it. There are a few ways to visualize ensemble data. The simplest is to just plot multiple instances of the line. Here we will just plot the temperature data against 50 random ensemble members. tana.ts.plot &lt;- plotTimeseriesEnsLines(X = tana.ae,Y = tana.temp,alp = 0.05,n.ens.plot = 50,color = &quot;blue&quot;) print(tana.ts.plot) Exercise 4.6 plotTimeseriesEnsLines() has options that control the output. Take a look at the documentation, and then change the following parameters, and understand how that affects the output: alp color (What does “Blues” or “Set2” do? How does it work) n.ens.plot Change the limits of the plot to only show the Holocene (~12,000-0 yr BP) Hint To change the limits, you’ll use the xlim() or scale_x_reverse() functions from ggplot2 4.3.5 Creating a timeseries plot with a ribbon confidence intervals We can also plot this as a ribbon plot of quantiles tana.ribbon.plot &lt;- plotTimeseriesEnsRibbons(X = tana.ae,Y = tana.temp) print(tana.ribbon.plot) Exercise 4.6 plotTimeseriesEnsRibbons () has many more options that control the output. Take a look at the documentation for that function, and then change the following parameters, and understand how that affects the output: probs color.high n.bins export.quantiles limit.outliers.x 4.3.6 Combining the two kinds of timeseries plots Each of these approaches to age-uncertain timeseries visualization is valuable - ribbons show case the probability distributions of the data through time, given the ensembles, but tend to smooth out real variability recorded by the data. Plotting an ensemble of lines highlights this variability, but gets messy with many lines, or doesn’t showcase the true range of uncertainty with a few lines. At different times, either approach can be right, but our favorite, standard approach is to do both, using the add.to.plot option we used above. Exercise 4.7 Use plotTimeseriesEnsRibbons(), plotTimeseriesEnsLines(), and the “add.to.plot” parameter to create a figure that shows the uncertainty ribbons in the background, and a with 5 ensemble members to highlight the variability of the data. Hint Something with this structure is what you’re looking for plotTimeseriesEnsRibbons() %&gt;% plotTimeseriesEnsLines() 4.4 Banded Age Modelling Thus far, we’ve explored methods to derive, transfer, and visualize age models derived from age-depth observations (mostly radiocarbon data). geoChronR also includes another type of age modelling algorithm, that estimates uncertainties based on miscounting rates in layer-counted archives. Consequently, BAM does not require, or even consider, depth in its uncertainty quantification. Although BAM is intended for layer-counted archives, like corals or varves, it can be useful as a rough approximation of age-uncertainty in non-banded timeseries, where the original geochronologic data needed to create a proper age model are not available. We’ll explore this with Tana Lake dataset and see how it compares. Comboul et al. (2014) is a probabilistic model of age errors in layer-counted chronologies. The model allows a flexible parametric representation of such errors (either as Poisson or Bernoulli processes), and separately considers the possibility of double-counting or missing a band. The model is parameterized in terms of the error rates associated with each event, which are intuitive parameters to geoscientists, and may be estimated via replication (DeLong et al. 2013). In cases where such rates can be estimated from the data alone, an optimization principle may be used to identify a more likely age model when a high-frequency common signal can be used as a clock (Comboul et al. 2014). As of now, BAM does not consider uncertainties about such parameters, representing a weakness of the method. BAM was coded in MATLAB, Python and R, and it is this latter version that geoChronR uses. tana &lt;- runBam(tana, paleo.meas.table.num = 1, n.ens = 1000, model.num = 5, make.new = TRUE, ens.table.number = 1, model = list(name = &quot;poisson&quot;, param = 0.05, resize = 0, ns = 1000)) ## [1] &quot;Found it! Moving on...&quot; tana.ye &lt;- selectData(tana,var.name = &quot;yearEnsemble&quot;,meas.table.num = 1) ## [1] &quot;Found it! Moving on...&quot; tana.ae.bam &lt;- convertAD2BP(tana.ye) tana.ribbon.plot.bam &lt;- plotTimeseriesEnsRibbons(X = tana.ae.bam,Y = tana.temp) #we can compare this to the original age model supplied by the paper (which used the Heegaard et al., 2005 model, so a whole other approach) tana.orig.age &lt;- selectData(tana,var.name = &quot;age&quot;,meas.table.num = 1) ## [1] &quot;Found it! Moving on...&quot; tana.ribbon.plot.bam &lt;- tana.ribbon.plot.bam + geom_line(aes(x = tana.orig.age$values, y = tana.temp$values),color = &quot;red&quot;) tana.ribbon.plot.bam library(egg) ggarrange(plots = list(tana.ribbon.plot + xlim(c(15000,0)) + ggtitle(&quot;Temperature on Multimodel age model&quot;), tana.ribbon.plot.bam + xlim(c(15000,0)) + ggtitle(&quot;Temperature on BAM&quot;)), nrow = 2) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. 4.5 Chapter project. This chapter covers a range of age modelling approaches, how to implement them in geoChronR, how to propagate the uncertainties to data of interest, and visualization of all of the above. Now it’s time to test what you’ve learned! In addition to a temperature reconstruction, the Tana Lake dataset includes \\(^{18}\\)O from leaf waxes. The data are measured on a different samples at different resolution, and so are stored in a different measurement table. So your project is: Exercise 4.8 Using tools learned in this chapter, create ensemble timeseries figures for \\(^{18}\\)O leaf wax from Tana Lake using, Bacon, Bchron, OxCal and BAM. Arrange these four figures vertically into a 4 panel figure with constant x-axes to allow comparison. How does the choice of age modelling algorithm impact the record? Which parts of the dataset are most vulnerable to age uncertainties? Hint Create plots for each model separately, and save them as variables. Then use ggarrange to combine them. References "],["correlation.html", "Chapter 5 Time-uncertain correlation 5.1 First look 5.2 How to correlate responsibly 5.3 The corEns() function 5.4 Plotting the ensemble correlation results 5.5 Judging the overall significance of an age-uncertain correlation 5.6 Chapter Project", " Chapter 5 Time-uncertain correlation Correlation is perhaps the most commonly used statistical tool in the paleogeosciences. This makes sense, because we often see similar patterns between datasets, but want to know whether the apparent relationship is robust, or could be spurious. And of course, age uncertainty can have immense impacts on correlation. In this chapter, we’ll explore the time-uncertain correlation tools in geoChronR by walking through a classic comparison in paleoclimatology, the relationship between \\(\\delta^{18}\\)O variability in Greenland ice cores and Asian speleothems. On multi-millennial timescales, the two datasets display such similar features that the well-dated Hulu Cave record, and other similar records from China and Greenland, have been used to argue for atmospheric teleconnections between the regions and support the independent chronology of GISP2 (Wang et al. 2001). Here, we revisit this relation quantitatively, using ensemble age models and the corEns function, to calculate the impact of age uncertainty on the correlation between these two iconic datasets. It’s worth noting at this point that there is a long and detailed discussion in the literature discussing this relationship, and any correlation approach to address this question ignores aspects of the science, and ignores ancillary evidence that may support a mechanistic relationship between two timeseries. Nevertheless, this is a good example of how age uncertainty can affect apparent alignment between two datasets. library(lipdR) library(geoChronR) library(ggplot2) library(magrittr) library(egg) 5.1 First look LiPD files for the Hulu Cave speleothem \\(\\delta^{18}\\)O and the GISP2 ice core \\(\\delta^{18}\\)O records are available at x and y. These LiPD files already include age ensembles, so you don’t need to create new age models. Use the skills you learned in Chapters 3 and @(agemodelling) to take a first look at the data. Exercise 5.1 Create a figure that that uses ensemble timeseries plots to compares the Hulu Cave GISP2 ice core records. Hint #1 Use readLipd() to load the data from the lipdverse Hint #2 Use mapAgeEnsembleToPaleoData(), and note that there is no depth data in the GISP2 dataset. Hint #3 Use selectData() to pull out the age ensembles and d18O values of interest Hint #4 Use plotEnsTimeseriesRibbons() and/or plotEnsTimeseriesLines() to create plots that span a common interval Hint #5 Use the egg package and ggarrange to stack the plots to allow easy comparison. Great! There’s no replacement for actually looking at the data, and lipdR and geoChronR make it possible to do this in just a few lines of code. If you found that exercise difficult, spend some time reviewing Chapters 3 and @(agemodelling). Exercise 5.2 Now that you’ve made the overview figure, does it look like there might be a relationship between these datasets? Would you expect the a positive, negative, or zero correlation? 5.2 How to correlate responsibly Before we jump into the details of how to conduct time-uncertain correlation in R, let’s review some of the key assumptions, methods and choices. The following is an excerpt from the geoChronR paper in Geochronology (McKay, Emile-Geay, and Khider 2021). Correlation is the most common measure of a relationship between two variables \\(X\\) and \\(Y\\). Its computation is fast, lending itself to ensemble analysis, with a handful of pretreatment and significance considerations that are relevant for ensembles of paleoenvironmental and geoscientific data. geoChronR implements three methods for correlation analysis: Pearson’s product-moment, Spearman’s rank and Kendall’s tau. Pearson correlation is the most common correlation statistic, but assumes normally-distributed data. This assumption is commonly not met in paleoenvironmental or geoscientific datasets, but can be can be overcome by mapping both datasets to a standard normal distribution prior to analysis Emile-Geay and Tingley (2016). Alternatively, the Spearman and Kendall correlation methods are rank-based, and do not require normally distributed input data, and are useful alternatives in many applications. 5.2.1 Binning All correlation analyses for timeseries are built on the assumption the datasets can be aligned on a common timeline. Age-uncertain data violate this assumption. We overcome this by treating each ensemble member from one or more age uncertain timeseries as valid for that iteration, then “bin” each of the timeseries into coeval intervals. The “binning” procedure in geoChronR sets up an interval, which is typical evenly spaced, over which the data are averaged. Generally, this intentionally degrades the median resolution of the timeseries, for example, a timeseries with 37-year median spacing could be reasonably “binned” into 100- or 200-year bins. The binning procedure is repeated for each ensemble member, meaning that between different ensembles, different observations will be placed in different bins. 5.2.2 Autocorrelation Following binning, the correlation is calculated and recorded for each ensemble member. The standard way to assess correlation significance is using a Student’s T-test, which assumes normality and independence. Although geoChronR can overcome the normality requirement, as discussed above, paleoenvironmental timeseries are often highly autocorrelated, and not serially independent, leading to spurious assessments of significance (Hu, Emile-Geay, and Partin 2017). geoChronR addresses this potential bias using three approaches: The simplest approach is to adjust the test’s sample size to reflect the reduction in degrees of freedom due to autocorrelation. Following Dawdy and Matalas (1964), the effective number of degrees of freedom is \\(\\nu = n \\frac{1-\\phi_{1,X}\\phi_{1,X}}{1+\\phi_{1,X}\\phi_{1,X}}\\), where \\(n\\) is the sample size (here, the number of bins) and where \\(\\phi_{1,X}, \\phi_{1,X}\\) are the lag-1 autocorrelation of two time series \\(X\\), \\(Y\\), respectively. This approach is called ``effective-n’’ in geoChronR. It is an extremely simple approach, with no added computations by virtue of being a parametric test using a known distribution (\\(t\\) distribution). A downside is that the correction is approximate, and can substantially reduce the degrees of freedom (Hu, Emile-Geay, and Partin 2017), to less than 1 in cases of high autocorrelation, which is common in paleoenvironmental timeseries. This may result in overly conservative assessment of significance, so this option is therefore not recommended. A parametric alternative is to generate surrogates, or random synthetic timeseries, that emulate the persistence characteristics of the series. This “isopersistent” test generates \\(M\\) (say, 500) simulations from an autoregressive process of order 1 (AR(1)), which has been fitted to the data. These random timeseries are then used to obtain the null distribution, and compute p-values, which therefore measure the probability that a correlation as high as the one observed (\\(r_o\\)) could have arisen from correlating \\(X\\) or \\(Y\\) with AR(1) series with identical persistence characteristics as the observations. This approach is particularly suited if an AR model is a sensible approximation to the data, as is often the case (Ghil et al. 2002). However, it may be overly permissive or overly conservative in some situations. A non-parametric alternative is the approach of Ebisuzaki (1997), which generates surrogates by scrambling the phases of \\(X\\) and \\(Y\\), thus preserving their power spectrum. To generate these “isospectral” surrogates, geoChronR uses the make_surrogate_data function from the rEDM package (Park et al. 2020). This method makes the fewest assumptions as to the structure of the series, and its computational cost is moderate, making it the default in geoChronR. 5.2.3 Test multiplicity In addition to the impact of autocorrelation on this analysis, repeating the test over multiple ensemble members raises the issue of test multiplicity (Ventura, Paciorek, and Risbey 2004), also known as the “look elsewhere effect.” To overcome this problem, we control for this false discovery rate (FDR) using the simple approach of Benjamini and Hochberg (1995), coded in R by Ventura, Paciorek, and Risbey (2004). FDR explicitly controls for spurious discoveries arising from repeatedly carrying out the same test. At a 5% level, one would expect a 1000 member ensemble to contain 50 spurious “discoveries” – instances of the null hypothesis (here “no correlation”) being rejected. FDR takes this effect into account to minimize the risk of identifying such spurious correlations merely on account of repeated testing. In effect, it filters the set of “significant” results identified by each hypothesis test (effective-N, isopersistent, or isospectral). 5.3 The corEns() function Now that you’ve thoroughly reviewed the theory, lets make it run in geoChronR! I know that not everyone thoroughly reviewed the theory. That’s ok, but it’s worth reflecting on these choices, because you won’t fully understand your results unless you understand these details. The default choices in geoChronR will not be right for every application. Let’s take another look at the the two timeseries over the period of overlap. To calculate age uncertain correlation let use geoChronR’s corEns function. I’m going to run this with 1000 ensemble members, but you may want to run it with only 200 ensemble members for now, since we have multiple significance testing options turned on and it can be a little slow. corout &lt;- corEns(time.1 = gisp2.ae, values.1 = gisp2.d18O, time.2 = hulu.ae, values.2 = hulu.d18O, bin.step = 200, max.ens = 1000, isopersistent = TRUE, isospectral = TRUE, gaussianize = TRUE) Hopefully, some of those options look familiar (see 5.2 if they don’t). There are a lot of choice that go into time uncertain correlation, meaning that there are a lot of parameters available for the corEns function (check out the help at ?corEns). Some of the key ones to pay attention to are: bin.step This variable sets the size of the bins that the data will be averaged into to align before correlation. Smaller bins will focus allow examination of shorter timescales, and increase the the sample size, but also increase the number of empty bins, and tend to increase autocorrelation. The right bin size varies depends on the distribution of resolutions in your datasets. isopersistent and isospectral These need to be set to TRUE if you want to calculate null significance models for significance testing with these methods. On the other hand, turning it off will speed up calculation if you don’t need it. gaussianize You’ll find the gaussianize option throughout geoChronR, and it’s typically the default option. gaussianize will transform the data into a Gaussian distribution, recognizing that many of the methods and/or their significance testing assume that the input data are normally distributed. This options ensures that this is the case. corEns returns a list that has the key correlation and significance results for all the selected methods. If percentiles are provided to corEns, and they are by default, the function will output a data.frame that summarizes the output. corout$cor.stats ## # A tibble: 5 x 6 ## percentiles r pSerial pRaw pIsopersistent pIsospectral ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.025 -0.454 0.117 0.000434 0.00000562 0.0279 ## 2 0.25 -0.269 0.389 0.0337 0.0414 0.189 ## 3 0.5 -0.192 0.544 0.145 0.155 0.382 ## 4 0.75 -0.115 0.722 0.368 0.389 0.614 ## 5 0.975 0.0466 0.959 0.859 0.894 0.937 5.4 Plotting the ensemble correlation results This gives us a first order sense of the results, but let’s use the plotCorEns function to dig in deeper. raw &lt;- plotCorEns(corout, significance.option = &quot;raw&quot;, use.fdr = FALSE)+ggtitle(&quot;Distribution of correlation coefficients&quot;) print(raw) Exercise 5.3 Explore the plotting options for plotCorEns. Change the color scheme Move the legend Move the labels Change which quantiles are plotted (this one is tricky) 5.4.1 Significance testing options In this figure we see the distribution of correlation coefficients, and their significance. Note that we chose “significance.option = ‘raw’,” so in green we see the distribution of significant correlations as determined by a standard T-test, without any consideration of autocorrelation. In this case, we observe that 31.7% of the correlations are significant. Of course we know this is a very simplistic approach, and that with many paleoclimate datasets we must consider the impact of temporal autocorrelation, which can readily cause spurious correlations. geoChronR addresses this point using three approaches, as discussed above in detail, and summarized here: The simplest approach (“eff-n”) is to adjust the test’s sample size to reflect the reduction in degrees of freedom due to autocorrelation. Alternatively, the “isopersistent” option will generate surrogates, or random synthetic timeseries, that emulate the persistence characteristics of the series, and use those to estiamte significance. The final option, “isospectral” also generates surrogates to estimate significance, but does so by scrambling the spectral phases of the two datasets, thus preserving their power spectrum while destroying the correlated signal. Let’s take a look at each of these three options. Exercise 5.4 Create three more versions of the correlation ensemble histogram, so that you have the original and one for all three options for accounting for autocorrelation. Label them, and combine them into a single plot. Which option shows the biggest reduction in significant correlations? Which is the most similar to the unadjusted distribution? If you put your plot together properly, you now see the dramatic effect of accounting for serial autocorrelation in our significance testing. Using the “effective-N” method drops the percentage of significant correlations (at the 0.05 level) to 0. However when autocorrelation is large, this approach dramatically reduces the effective degrees of freedom and has been shown to be overly conservative in many cases. So let’s take a look at the surrogate-based approaches. Using the “isopersistent” approach, where we simulate thousands of synthetic correlations with the same autocorrelation characteristics as the real data, and see how frequently we observe r-values at the observed levels, gives a much less conservative result. In the isospectral test, only a few of the ensembles members are significant. This approach often serves as a compromise between the more conservative effective-N approach and the more liberal isopersistent approach. The isospectral method also makes the fewest assumptions as to the structure of the series, and its computational cost is moderate, and so it is the default in geoChronR. 5.4.2 False-discovery rate testing Although taking advantage of the age ensembles allows us to propagate the impacts of age uncertainty, it introduces another statistical problem on our hands. In addition to the impact of autocorrelation on this analysis, repeating the test over multiple ensemble members raises the issue of test multiplicity (Ventura, Paciorek, and Risbey 2004), or the “look elsewhere effect.” At a 5% significance level, one would expect a 1000 member ensemble to contain 50 spurious “discoveries” – instances of the null hypothesis, here “no correlation” being rejected. To overcome this problem, we control for this false discovery rate (FDR) using the simple approach of Benjamini and Hochberg (1995), coded in R by Ventura, Paciorek, and Risbey (2004). FDR takes this effect into account to minimize the risk of identifying such spurious correlations merely on account of repeated testing. In effect, it filters the set of “significant” results identified by each hypothesis test (effective-N, isopersistent, or isospectral). Let’s plot the results of the isopersistent test again, but turn the use.fdr option to TRUE. isoPersistentFdr &lt;- plotCorEns(corout, legend.position =c(.85,.8), f.sig.lab.position = c(.85,.6), significance.option = &quot;isopersistent&quot;, use.fdr = TRUE)+ggtitle(&quot;Isopersistent significance testing with FDR&quot;) print(isoPersistentFdr) Now we see how accounting for FDR can further reduce our significance levels. In this case many of the significant correlations are expected due to test multiplicity (shown in the green bars in the plot above). This represents the randomness that we’re sampling by repeating the correlation across 1000 ensemble members. After accounting for this, only 33.6% of the ensemble members are significant. Note that accounting for False Discovery Rate is a separate process than deriving p-values, and can be applied to any of the significance.options in geoChronR. 5.5 Judging the overall significance of an age-uncertain correlation So, with all of the methods, only a small subset of the correlations are significant, so it’s probably fair to say to that this is not a significant correlation after accounting for age uncertainty. But this begs the question, what fraction of the correlation ensemble needs to be significant to consider an age-uncertain relation significant? There is no hard and fast theoretical justification for what fraction of ensemble correlation results should be expected to pass such a significance test, and so evaluating the significance of age uncertain correlation remains somewhat subjective. Indeed, two truly correlated timeseries, when afflicted with age uncertainty, will commonly return some fraction of insignificant results when random ensemble members are correlated against each other. The frequency of these “false negatives” depends on the structure of the age uncertainties and the timeseries, and will vary to some extent by random chance. One way to get a sense of the vulnerability of a timeseries to false negatives is to perform an age-uncertain correlation of a dataset with itself. Exercise 5.5 Calculate the correlation ensemble where you correlate the Hulu Cave \\(\\delta^{18}\\)O record with itself, and plot a histogram of the results. Use the isospectral method to quantify significance, while accounting for FDR. Repeat this exercise with the GISP2 \\(\\delta^{18}\\)O record. Make a combined plot, with the axes aligned. What fraction of significant correlations do you get with this approach for each record? Does one dataset have higher self-correlations that the other? Explore the data to think what might cause this? 5.6 Chapter Project Generally, age uncertainties obscure relationships between records, while in rare cases creating the appearance of spurious correlations. It is appropriate to think of ensemble correlation as a tool to explore the age-uncertain correlation characteristics between timeseries, rather than a binary answer to the question “Are these two datasets significantly correlated?” So for your chapter project, we’ll explore one more parameter that will have significant impacts on your result. Exercise 5.6 OK, let’s go through the whole exercise of comparing two time-uncertain series. This time, we’re going to look on a shorter timescale, just the past 2000 years. Your project is to conduct an age-uncertain correlation of two ice core d18O records, from the Renland and Crete ice cores. LiPD data with age ensembles are available for Renland and Crete. You’ll need to go through the whole process explored in this chapter, but once you’ve produced an age uncertain correlation, explore the impact of changing the bin.step over a range of reasonable choices. How do larger (and smaller) bins affect the correlation, and the significance of the result? Defend the choice that you think is most reasonable. References "],["principal-components-analysis-pca.html", "Chapter 6 Principal Components Analysis {pca} 6.1 Make a map 6.2 Calculate the ensemble PCA 6.3 Ensemble PCA using a correlation matrix. 6.4 Chapter project", " Chapter 6 Principal Components Analysis {pca} As always, let’s start by loading the packages we need. library(lipdR) library(geoChronR) library(magrittr) library(dplyr) library(purrr) library(ggplot2) This vignette showcases the ability to perform principal component analysis (PCA, also known as empirical orthogonal function (EOF) analysis. Data are from the PalMod compilation. We’ll just load a subset of it here, but the rest is available on the LiPDverse() FD &lt;- lipdR::readLipd(&quot;http://lipdverse.org/geoChronR-examples/PalMod-IndianOcean/PalMod-IndianOcean.zip&quot;) ## [1] &quot;reading: 182_1132B.lpd&quot; ## [1] &quot;reading: GeoB12615_4.lpd&quot; ## [1] &quot;reading: GIK17940_2.lpd&quot; ## [1] &quot;reading: GIK17961_2.lpd&quot; ## [1] &quot;reading: GIK18471_1.lpd&quot; ## [1] &quot;reading: MD01_2378.lpd&quot; ## [1] &quot;reading: MD02_2589.lpd&quot; ## [1] &quot;reading: MD06_3067.lpd&quot; ## [1] &quot;reading: MD06_3075.lpd&quot; ## [1] &quot;reading: MD73_025.lpd&quot; ## [1] &quot;reading: MD84_527.lpd&quot; ## [1] &quot;reading: MD88_770.lpd&quot; ## [1] &quot;reading: MD98_2181.lpd&quot; ## [1] &quot;reading: ODP1145.lpd&quot; ## [1] &quot;reading: SK157_14.lpd&quot; ## [1] &quot;reading: SO42_74KL.lpd&quot; ## [1] &quot;reading: WIND_28K.lpd&quot; 6.1 Make a map First, let’s take a quick look at where these records are located. geoChronR’s mapLipd function can create quick maps: mapLipd(FD,map.type = &quot;line&quot;,f = 0.1) 6.1.1 Grab the age ensembles for each record. Now we need to map the age ensembles to paleo for all of these datasets. We’ll use purrr::map for this, but you could also do it with sapply(). In this case we’re going to specify that all of the age ensembles are named “ageEnsemble,” and the chron and paleo depth variables. Fortunately, the naming is consistent across PalMod, making this straightforward. FD2 = purrr::map(FD, mapAgeEnsembleToPaleoData, strict.search = TRUE, paleo.depth.var = &quot;depth_merged&quot;, chron.depth.var = &quot;depth&quot;, age.var = &quot;ageEnsemble&quot; ) 6.1.2 Filter the data In this analysis, we’re interested in spatiotemporal patterns in the Indian Ocean during the Last Deglaciation. Not all of these records cover that time interval, so let’s take a look at the temporal coverage. indTib &lt;- FD2 %&gt;% extractTs() %&gt;% ts2tibble() #create a lipd-timeseries-tibble #use purrr to extract the minimum and maximum ages for each record minAge &lt;- map_dbl(indTib$age,min,na.rm = TRUE) maxAge &lt;- map_dbl(indTib$age,max,na.rm = TRUE) #plot the distributions of the ages. ggplot()+ geom_histogram(aes(x = minAge,fill = &quot;Min age&quot;)) + geom_histogram(aes(x = maxAge,fill = &quot;Max age&quot;),alpha = 0.5) + scale_fill_manual(&quot;&quot;,values = c(&quot;red&quot;,&quot;black&quot;)) + xlab(&quot;Age&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. OK, so it looks like the ages range from 0 to about 150. But what are the units on those? Exercise 6.1 Explore the LiPD data to determine the units on the ages, and update your histogram appropriately. Since we’re interested in the deglaciation, we need to find which datasets span the range from at least 10,000 to 30,000 years ago, and that have enough data in that range to be useful. There are a few ways to go about this. Here’s one: indTs &lt;- extractTs(FD2) #create a lipd-timeseries # create some variables for screening startYear &lt;- 10 endYear &lt;- 30 #write a function to determine if the dataset has enough values within the target time frame nGoodVals &lt;- function(x,startYear,endYear){ agesWithValues &lt;- x$age[is.finite(x$paleoData_values)] gv &lt;- which(agesWithValues &gt;= startYear &amp; agesWithValues &lt;= endYear) return(length(gv)) } #write a function to determine how much coverage the data has within that target frame span &lt;- function(x,startYear,endYear){ agesWithValues &lt;- x$age[is.finite(x$paleoData_values)] agesWithValues &lt;- agesWithValues[agesWithValues &gt;= startYear &amp; agesWithValues &lt;= endYear] sout &lt;- abs(diff(range(agesWithValues,na.rm = TRUE))) return(sout) } #use purrr to run those functions over each item in the list nValsInRange &lt;- map_dbl(indTs,nGoodVals,startYear,endYear) span &lt;- map_dbl(indTs,span,startYear,endYear) OK, now we can see for each of these timeseries how many values are between 10 and 30 ka, and how much time range that span corresponds to. We can now filter our TS object down to something closer to what we’re looking for, and then select on the variable we’re looking for, in this, “surface.temp” #use our indices from above to select only the good timeseries TS.filtered &lt;- indTs[nValsInRange &gt; 20 &amp; span &gt; 15] %&gt;% filterTs(&quot;paleoData_variableName == surface.temp&quot;) #and then select our variable of interest ## [1] &quot;14 results after query: paleoData_variableName == surface.temp&quot; At this point, we should get our eyes on the data. Let’s make a timeseries plot of the existing data. tsTib &lt;- ts2tibble(TS.filtered) tp &lt;- tidyTs(tsTib,age.var = &quot;age&quot;) #filter it only to our time range tp &lt;- tp %&gt;% filter(between(age,10,30)) plotTimeseriesStack(tp,time.var = &quot;age&quot;) OK, it looks like our filtering worked as expected. We now have a lipd-timeseries object that contains only the timeseries we want to include in our PCA, and the age ensembles are included. We’re ready to move forward with ensemble PCA! To conduct PCA, we need to put all of these data onto a common timescale. We will use binning to do this, although there are also other approaches. We also want to repeat the binning across our different ensemble members, recognizing that age uncertainty affects the bins! binTs will bin all the data in the TS from 10 ka to 23 ka into 5 year bins. binned.TS &lt;- binTs(TS.filtered,bin.vec = seq(10,30,by=.5),time.var = &quot;ageEnsemble&quot;) We’re now ready to calculate the PCA! 6.2 Calculate the ensemble PCA Calculate PCA on each ensemble member: pcout &lt;- pcaEns(binned.TS,pca.type = &quot;cov&quot;) That was easy (because of all the work we did beforehand). But before we look at the results let’s take a look at a scree plot to get a sense of how many significant components we should expect. plotScreeEns(pcout) It looks like the first two components, shown in black with gray uncertainty shading, stand out above the null model (in red), but the third and beyond look marginal to insignficant. Let’s focus on the first two components. 6.2.1 Plot the ensemble PCA results Now let’s visualize the results. The plotPcaEns function will create multiple diagnostic figures of the results, and stitch them together. plotPCA &lt;- plotPcaEns(pcout, TS = TS.filtered, map.type = &quot;line&quot;, f=.1, legend.position = c(0.5,.6), which.pcs = 1:2, which.leg = 2) Nice! A summary plot that combines the major features is produced, but all of the components, are included in the “plotPCA” list that was exported. For comparison with other datasets it can be useful to export quantile timeseries shown in the figures. plotTimeseriesEnsRibbons() can optionally be used to export the data rather than plotting them. The following will export the PC1 timeseries: quantileData &lt;- plotTimeseriesEnsRibbons(X = pcout$age,Y = pcout$PCs[,1,],export.quantiles = TRUE) print(quantileData) ## # A tibble: 200 x 6 ## ages `0.025` `0.25` `0.5` `0.75` `0.975` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.8 5.05 6.01 6.62 7.24 8.32 ## 2 10.8 5.24 6.05 6.52 7.05 7.96 ## 3 10.9 5.39 6.06 6.43 6.82 7.58 ## 4 11.0 5.51 6.05 6.34 6.66 7.25 ## 5 11.1 5.56 5.99 6.26 6.56 7.02 ## 6 11.2 5.44 5.86 6.19 6.52 6.91 ## 7 11.3 5.35 5.78 6.08 6.35 6.79 ## 8 11.4 5.33 5.74 5.96 6.17 6.57 ## 9 11.5 5.26 5.67 5.82 6.02 6.41 ## 10 11.6 5.08 5.54 5.71 5.89 6.34 ## # … with 190 more rows 6.3 Ensemble PCA using a correlation matrix. Let’s repeat much of this analysis, but this time let’s take a look at the data underlying the surface temperature reconstructions, \\(\\delta^{18}O\\) and Mg/Ca data. First, look at the variable names represented in our dataset. This is easiest with our tibble tib.filtered &lt;- indTs[nValsInRange &gt; 20 &amp; span &gt; 15] %&gt;% ts2tibble() table(tib.filtered$paleoData_variableName) ## ## age ageEnsemble benthic.d13C benthic.d18O CaCO3 DBD depth depth_merged ## 28 11 5 7 2 1 1 11 ## planktonic.d13C planktonic.d18O planktonic.MgCa surface.temp TOC UK37 ## 8 13 8 14 3 3 OK. Let’s filter the timeseries again, this time pulling all the \\(\\delta^{18}O\\) and Mg/Ca data. Exercise 6.2 Write code to filter tib.filtered to only includes the planktonic d18O and \\(\\delta^{18}O\\) and Mg/Ca data. Great. It’s pretty easy to filter the data if you’re used to dplyr, and if you’re not, it’s not too hard to learn. Now let’s take a look at the data. Exercise 6.3 Create a plotStack visualizing these data. Color the timeseries by variable name. Take a look at your plot, note the units, and the scale of the variability. Since we’re now dealing with variables in different units, we cannot use a covariance matrix. And calculate the ensemble PCA, this time using a covariance matrix. By using a covariance matrix, we’ll allow records that have larger variability in \\(\\delta^{18}O\\) to influence the PCA more, and those that have little variability will have little impact. This may or may not be a good idea, but it’s an important option to consider when possible. pcout2 &lt;- pcaEns(binned.TS2,pca.type = &quot;corr&quot;) Once again, let’s take a look at the scree plot: plotScreeEns(pcout2) Once again, the first PC dominates the variability. All of the subsequent PCs are non signficant, but let’s include the second third PC in our plot this time as well. Exercise 6.4 Create a PCA plot showing the output of your data. This time let’s explore the options. Show the first 3 PCs Select a different map background Change the shapes Change the color scale. Again, the scree plot tells us that only the first EOF pattern is worth interpreting, so let’s interpret it! Is the timeseries more or less what you expected? How does it compare from what we got from the surface temperature data? The spatial pattern show high positive and high negative values? How does this compare with our previous analysis? What might be the origin of this pattern? 6.4 Chapter project Exercise 6.5 For your chapter project for PCA, we’re going to take a bigger picture look at planktonic \\(\\delta^{18}O\\). Specifically, I’d like you to conduct time-uncertain PCA on global planktonic \\(\\delta^{18}O\\) data that span from 130ka up to the present. I’ve prepared a subset of the compilation that has at least 100 observations between 130 ka and the present, and that spans at least 100 kyr during that interval, which is a great start. That subset is here. You’ll have to make some decisions along the way. Go ahead and make your choices, just have a good reason for each choice you make! "],["regression-and-calibration-in-time.html", "Chapter 7 Regression and Calibration-in-time 7.1 Load the LiPD file 7.2 Check out the contents 7.3 Create an age model with Bacon 7.4 And plot the ensemble output 7.5 Map the age ensemble to the paleodata table 7.6 Select the paleodata age ensemble, and RABD data that we’d like to regress and calibrate 7.7 Now load in the instrumental data we want to correlate and regress agains 7.8 Check age/time units before proceeding 7.9 Create a “variable list” for the instrumental data 7.10 Calculate an ensmeble correlation between the RABD and local summer temperature data 7.11 And plot the output 7.12 Perform ensemble regression 7.13 And plot the output 7.14 Time-uncertain spectral analysis 7.15 Conclusion", " Chapter 7 Regression and Calibration-in-time In this chapter, we will replicate the analysis of Boldt et al. (2015), performing age-uncertain calibration-in-time on a chlorophyll reflectance record from northern Alaska, using geoChronR. The challenge of age-uncertain calibration-in-time is that age uncertainty affects both the calibration model (the relation between the proxy data and instrumental data) and the reconstruction (the timing of events in the reconstruction). geoChronR simplifies handling these issues. Let’s start by loading the packages we’ll need. library(lipdR) #to read and write LiPD files library(geoChronR) #of course library(readr) #to load in the instrumental data we need library(ggplot2) #for plotting 7.1 Load the LiPD file OK, we’ll begin by loading in the Kurupa Lake record from Boldt et al., 2015. The system.file(...) part of this pulls the example file from the package directory. You’d like just enter the path as a string for typical use. K &lt;- lipdR::readLipd(&quot;http://lipdverse.org/geoChronR-examples/Kurupa.Boldt.2015.lpd&quot;) ## [1] &quot;reading: Kurupa.Boldt.2015.lpd&quot; 7.2 Check out the contents sp &lt;- plotSummary(K,paleo.data.var = &quot;RABD&quot;,summary.font.size = 6) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; print(sp) ## TableGrob (4 x 4) &quot;arrange&quot;: 4 grobs ## z cells name grob ## 1 1 (1-1,1-2) arrange gTree[GRID.gTree.2493] ## 2 2 (1-2,3-4) arrange gtable[layout] ## 3 3 (2-4,1-2) arrange gtable[layout] ## 4 4 (3-4,3-4) arrange gtable[layout] 7.3 Create an age model with Bacon K &lt;- runBacon(K, lab.id.var = &#39;labID&#39;, age.14c.var = &#39;age14C&#39;, age.14c.uncertainty.var = &#39;age14CUncertainty&#39;, age.var = &#39;age&#39;, age.uncertainty.var = &#39;ageUncertainty&#39;, depth.var = &#39;depth&#39;, reservoir.age.14c.var = NULL, reservoir.age.14c.uncertainty.var = NULL, rejected.ages.var = NULL, bacon.acc.mean = 10, bacon.thick = 7, ask = FALSE, bacon.dir = &quot;~/Cores&quot;, suggest = FALSE, close.connection = FALSE) ## Using a mix of cal BP and calibrated C-14 dates ## Warning! The file with the dates seems newer than the run you are loading. If any dates have been added/changed/removed?, then please run Bacon.cleanup() ## Warning, this will take quite some time to calculate. I suggest increasing d.by to, e.g.10 ## Calculating age ranges... ## ## Preparing ghost graph... ## ## Mean 95% confidence ranges 318 yr, min. 4 yr at 0 cm, max. 520 yr at 294 cm ## 100% of the dates overlap with the age-depth model (95% ranges) ## 7.4 And plot the ensemble output plotChron(K,age.var = &quot;ageEnsemble&quot;,dist.scale = 0.2) ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;plotting your chron ensemble. This make take a few seconds...&quot; ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. 7.5 Map the age ensemble to the paleodata table This is to get ensemble age estimates for each depth in the paleoData measurement table K &lt;- mapAgeEnsembleToPaleoData(K,age.var = &quot;ageEnsemble&quot;) ## [1] &quot;Kurupa.Boldt.2015&quot; ## [1] &quot;Looking for age ensemble....&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;Found it! Moving on...&quot; ## [1] &quot;getting depth from the paleodata table...&quot; ## [1] &quot;Found it! Moving on...&quot; 7.6 Select the paleodata age ensemble, and RABD data that we’d like to regress and calibrate kae &lt;- selectData(K,&quot;ageEnsemble&quot;) ## [1] &quot;Found it! Moving on...&quot; rabd &lt;- selectData(K,&quot;RABD&quot;) ## [1] &quot;Found it! Moving on...&quot; 7.7 Now load in the instrumental data we want to correlate and regress agains kurupa.instrumental &lt;- readr::read_csv(&quot;http://lipdverse.org/geoChronR-examples/KurupaInstrumental.csv&quot;) ## Rows: 134 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): Year (AD), JJAS Temperature (deg C) ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 7.8 Check age/time units before proceeding kae$units ## [1] &quot;yr BP&quot; yep, we need to convert the units from BP to AD kae &lt;- convertBP2AD(kae) 7.9 Create a “variable list” for the instrumental data kyear &lt;- list() kyear$values &lt;- kurupa.instrumental[,1] kyear$variableName &lt;- &quot;year&quot; kyear$units &lt;- &quot;AD&quot; kinst &lt;- list() kinst$values &lt;- kurupa.instrumental[,2] kinst$variableName &lt;- &quot;Temperature&quot; kinst$units &lt;- &quot;deg (C)&quot; 7.10 Calculate an ensmeble correlation between the RABD and local summer temperature data corout &lt;- corEns(kae,rabd,kyear,kinst,bin.step=2,percentiles = c(.05,.5,.95 )) 7.11 And plot the output Note that here we use the “Effective-N” significance option as we mimic the Boldt et al. (2015) paper. plotCorEns(corout,significance.option = &quot;eff-n&quot;) Mixed results. But encouraging enough to move forward. 7.12 Perform ensemble regression OK, you’ve convinced yourself that you want to use RABD to model temperature back through time. We can do this simply (perhaps naively) with regession, and lets do it with age uncertainty, both in the building of the model, and the reconstructing regout &lt;- regressEns(time.x = kae, values.x = rabd, time.y =kyear, values.y =kinst, bin.step=3, gaussianize = FALSE, recon.bin.vec = seq(-4010,2010,by=20)) 7.13 And plot the output regPlots &lt;- plotRegressEns(regout,alp = 0.01,font.size = 8) This result is consistent with that produced by Boldt et al., (2015), and was much simpler to produce with geoChronR. &lt;!--chapter:end:regression.Rmd--&gt; # Spectral Analysis In this notebook we demonstrate how to use the spectral analysis features of GeoChronR using the &quot;ODP846&quot; record described in: - Mix, A. C., J. Le, and N. J. Shackleton (1995a), Benthic foraminiferal stable isotope stratigraphy from Site 846: 0–1.8 Ma, Proc. Ocean Drill. Program Sci. Results, 138, 839–847. - Shackleton, N. J. (1995), New data on the evolution of Pliocene climate variability, in Paleoclimate and Evolution, With Emphasis on Human Origins, edited by E. S. Vrba et al., pp. 242-248, Yale Univ. Press, New Haven, CT. The data were aligned to the Benthic Stack of [Lisiecki &amp; Raymo (2005)](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2004PA001071) using the [HMM-Match](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2014PA002713) algorithm [(Khider et al, 2017)](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016PA003057). The latter is a probabilistic method that generates an ensemble of 1000 possible age models compatible with the chronostratigraphic constraints. We first remotely load the data using the LiPD utilities. Depending on your connectivity, this may take a minute as the file is fairly large (2.6Mb). ```r library(lipdR) # to load the file library(geoChronR) # to analyze it library(ggthemes) # to define plotting theme library(ggplot2) # to plot I &lt;- readLipd(&quot;http://lipdverse.org/geoChronR-examples/ODP846.Lawrence.2006.lpd&quot;) Now let us take a first look at the median age model: d18O = I$chronData[[1]]$model[[1]]$summaryTable[[1]]$d180$values t.med &lt;- I$chronData[[1]]$model[[1]]$summaryTable[[1]]$median$values L &lt;- geoChronR::mapAgeEnsembleToPaleoData(I,paleo.meas.table.num = 1,age.var = &quot;age&quot;) age = geoChronR::selectData(L,&quot;ageEnsemble&quot;,meas.table.num = 1) temp = geoChronR::selectData(L,&quot;temp muller&quot;,meas.table.num = 1) plotTimeseriesEnsRibbons(ggplot(),X=age,Y=temp,color.low=&quot;orchid&quot;,color.high=&quot;darkorange3&quot;,color.line=&quot;orange&quot;,line.width=0.5,alp=0.3) + scale_x_reverse() + theme_hc(style = &quot;darkunica&quot;) This paleoclimate record features: a long-term cooling trend (\\(\\delta^{18}\\mathrm{O}\\) gets more positive over time) characteristic of late Neogene and Quaternary. some quasi-periodic oscillations (the legendary Pleistocene Ice Ages) nonstationary behavior, related to the well-known mid-Pleistocene transition from a “41k world” to a “100k world” somewhere around 0.8 Ma (Paillard, 2001). To keep things simple and lower computational cost, let’s focus on the last million years, and use the median age model. Now, a standard assumption of spectral analysis is that data are evenly spaced in time. In real-world paleo timeseries this is seldom the case. Let’s look at the distribution of time increments in this particular core, as contrained by this tuned age model: age.median = matrixStats::rowQuantiles(age$values,probs = 0.5) temp.median = matrixStats::rowQuantiles(as.matrix(temp$values),probs = 0.5) t &lt;-age.median[age.median &lt; 1000] X &lt;- temp.median[age.median &lt; 1000] X &lt;- X - mean(X) #dfs = dplyr::filter(df,t&lt;=1000) dt = diff(t) ggplot() + xlim(c(0,10)) + geom_histogram(aes(x=dt,y = ..density..), bins = 25, ,alpha = 0.8, fill = &quot;orange&quot;) + ggtitle(&quot;Distribution of time intervals&quot;) + theme_hc(style = &quot;darkunica&quot;) + xlab(expression(Delta*&quot;t&quot;)) We see that over the past 1 Ma, the time increments (\\(\\Delta t\\)) are sharply peaked around 2 ka, but they range from 0 to about 7.5 ka. For now, let us assume that the time axis, albeit uneven, is well-known (no uncertainty). ## Time-certain spectral analysis From this point there are two ways to proceed: 1) use methods that explictly deal with unevenly-spaced data, or 2) interpolate to a regular grid and apply standard methods. In the first case, we could use the Lomb-Scargle periodogram or rather its version tailored for paleoclimate data, REDFIT. In the second case, we can interpolate and use a method tailored to evenly-spaced data. 7.13.1 Lomb-Scargle periodogram This is a very standard method implemented in many packages. For a review, see VanderPlas (2018). There are several ways to implement Lomb-Scargle. geoChronR does this via the lomb package. To establish significance, we have a few choices. We re-use Stephen Meyer’s excellent astrochron package for this purpose, as it implements the methods described in Meyers, (2012). Several nulls could be chosen here, and we focus first on a power-law, characteristic of many paleoclimate records (e.g. Zhu et al, 2019). spec.ls &lt;- computeSpectraEns(t,X,method = &#39;lomb-scargle&#39;) ls.df &lt;- data.frame(&quot;freq&quot; = spec.ls$freqs, &quot;pwr&quot; = spec.ls$power) # estimate significance against a power-law fit f.low &lt;- 1e-3; f.high &lt;- 0.1 plaw.ls &lt;- astrochron::pwrLawFit(ls.df, dof = 2, flow = f.low, fhigh = f.high, output = 1, genplot = F) cl.df &lt;- data.frame(plaw.ls[,union(1,c(5:7))]) # extract confidence limits # rename columns to be less silly names(cl.df)[1] &lt;- &quot;freq&quot; names(cl.df)[2] &lt;- &quot;90% CL&quot; names(cl.df)[3] &lt;- &quot;95% CL&quot; names(cl.df)[4] &lt;- &quot;99% CL&quot; # plot this pticks = c(10, 20, 50, 100, 200, 500, 1000) prange = c(10,1000) yl = c(0.01,1000) p.ls &lt;- plotSpectrum(ls.df,cl.df,x.lims = prange,x.ticks = pticks, y.lims = yl, color.line=&#39;orange&#39;, color.cl=&#39;white&#39;) + ggtitle(&quot;IODP 846 d18O, Lomb-Scargle periodogram&quot;) + theme_hc(style = &quot;darkunica&quot;) + theme(axis.ticks.x = element_line(color = &quot;gray&quot;)) # label periodicities of interest p.ls &lt;- periodAnnotate(p.ls, periods = c(19,23,41,100), y.lims =c(0.01,100)) show(p.ls) It is clearly seen that the data contain significant energy (peaks) near, but not exactly at, the famed Milankovitch periodicities (100, 41, 23, and 19 kyr). These periodicities (particularly the eccentricity (100kyr) and obliquity (40kyr)) rise above the various power law nulls, but we see hints of higher power at high-frequencies. We shall soon see that this is an artifact of the Lomb-Scargle methods, which does not use any tapers and therefore displays high variance (in this context, spurious peaks). Other methods smooth out that noise. REDFIT does so via application of Welch’s Overlapped Segment Averaging, which by default uses 3 segments that overlap by about 50%. geoChronR uses its dplR implementation, which differs slightly from the published algorithm (see ?dplR::redfit for details). spec.redfit &lt;- computeSpectraEns(t,X,method = &#39;redfit&#39;) redfit.df &lt;- data.frame(&quot;freq&quot; = spec.redfit$freq, &quot;pwr&quot; = spec.redfit$power) cl.df &lt;- data.frame(&quot;freq&quot; = spec.redfit$freq, &quot;95% CL&quot; = spec.redfit$power.CL) names(cl.df)[2] &lt;- &quot;95% CL&quot; # plot this pticks = c(10, 20, 50, 100, 200, 500, 1000) prange = c(10,1000) yl = c(0.01,1000) p.ls &lt;- plotSpectrum(redfit.df,cl.df,x.lims=prange,x.ticks = pticks, y.lims = yl, color.line=&#39;orange&#39;, color.cl=&#39;white&#39;) + ggtitle(&quot;IODP 846 d18O, REDFIT estimation&quot;) + theme_hc(style = &quot;darkunica&quot;) + theme(axis.ticks.x = element_line(color = &quot;gray&quot;)) # label periodicities of interest p.ls &lt;- periodAnnotate(p.ls, periods = c(19,23,41,100), y.lims =c(0.01,100)) show(p.ls) Now, this clearly is smoother, perhaps a little too much. One could play with n50, the size of the window, or iwin, its shape, to reduce the smoothing. There are still peaks near the Milankovitch periodicities, but in many cases they do not stand out (cf 41 kyr). 7.13.2 Multi-taper Method The Lomb-Scargle periodogram is a decent way to deal with unevenly-spaced timeseries, but it is still a periodogram, which has several problems. In particular, it is inconsistent: the variance of each estimate goes to infinity as the number of observations increases. A much better estimator is Thomson’s Multi-Taper Method Thomson, 1982, which is consistent (the more data you have, the better you know the spectrum, as it should be). Formally, MTM optimizes the classic bias-variance tradeoff inherent to all statistical inference. It does so by minimizing leakage outside of a frequency band with half-bandwidth equal to \\(pf_R\\), where \\(f_R=1/(N \\Delta t)\\) is the Rayleigh frequency, \\(\\Delta t\\) is the sampling interval, \\(N\\) the number of measurements, and \\(p\\) is the so-called time-bandwidth product. \\(p\\) can only take a finite number of values, all multiples of 1/2 between 2 and 4. A larger \\(p\\) means lower variance (i.e. less uncertainty about the power), but broader peaks (i.e. a lower spectral resolution), synonymous with more uncertainty about the exact location of the harmonic. So while MTM might not distinguish between closely-spaced harmonics, it is much less likely to identify spurious peaks, especially at high frequencies. In addition, a battery of formal tests have been devised with MTM, allowing under reasonably broad assumptions to ascertain the significance of spectral peaks. We show how to use this “harmonic F-test” below. A notable downside of MTM is that it only handles evenly-spaced data. Small potatoes! As we saw above, the data are not that far from evenly-spaced, so let’s interpolate and see what we get. Conveniently, both the interpolation routine and MTM are available within the astrochron package. library(astrochron) dfs = data.frame(time=t,temp=X) dti = 2.5 # interpolation interval, corresponding to the mode of the \\Delta t distribution dfe = linterp(dfs,dt=dti) tbw = 2 #time-bandwidth product for the analysis; we use the minimum allowed to limit smoothing. Most of the astrochron routines produce a diagnostic graphical output, which you can silence by turning the genplot flag to FALSE. However, it is instructive to take a peak at the top-left panel and see where the interpolation has made a difference. We see that the black line closely espouses the red measurements for most of the timeseries, reassuring us that this process did not introduce spurious excursions or oscillations. Now let’s compute the spectrum using MTM on the equally-spaced data stored in dfe. We continue to manually label Milankovitch frequencies (in orange) and label in green (technically, “chartreuse”) the periodicities identified as significant by the test. Here, we start with the default option of a test against an AR(1) background. spec.mtm &lt;- computeSpectraEns(dfe$time,dfe$temp,method = &#39;mtm&#39;, tbw=tbw) # tbw is the time-bandwidth product, p in the above sig.freq &lt;- astrochron::mtm(dfe,tbw=tbw, padfac=5,ar1=TRUE,genplot = F,output=2, verbose = F, detrend=T) mtm.df &lt;- data.frame(&quot;freq&quot; = spec.mtm$freqs, &quot;pwr&quot; = spec.mtm$power) # plot this prange = c(5,1000) p.mtm &lt;- plotSpectrum(mtm.df,x.lims=prange,x.ticks = pticks, y.lims = c(1e-6,10), color.line=&#39;orange&#39;) + ggtitle(&quot;IODP 846 d18O, Multi-taper method, AR(1) null&quot;) + xlab(&quot;Period (ky)&quot;) + theme_hc(style = &quot;darkunica&quot;) + theme(axis.ticks.x = element_line(color = &quot;gray&quot;)) # label periodicities of interest p.mtm &lt;- periodAnnotate(p.mtm, periods = c(19,23,41,100), y.lims = c(1e-6,1)) p.mtm &lt;- periodAnnotate(p.mtm, periods = 1/sig.freq$Frequency,color = &quot;chartreuse&quot;,y.lims = c(1e-6,.1)) show(p.mtm) You may notice a few differences to the Lomb-Scargle periodogram. First, the high frequency part is much smoother, getting rid of a lot of high-frequency noise. There is also a clear power law behavior from periods of 5 to 100 ky, which in this log-log plotting convention manifests as a linear decrease. Astrochron implements several tests to detect harmonic (sinusoidal) components. Like all tests, they are heavily dependent on a null hypothesis. By default, astrochron::mtm() assumes that we test against an AR(1) model. Using the option output = 2, it will export the frequencies identified as “significant” by this procedure. In this case, it roughly identifies the Milankovitch frequencies we expected to find, plus many others. What should we make of that? The 100ka cycle is now labeled as 90 ka, though given the peak’s width, one should not be foolish enough to consider them distinct. There is also evidence that this peak may be the result of ice ages clustering every 2 to 3 obliquity cycles (81 or 123 ka), averaging around 100 over the pleistocene Huybers &amp; Wunsch (2005). Bottom line: with such spectral resolution, 94 and 100 are one and the same, and may really be a mixture of 80 and 120. Overall, this test identifies 8 periodicities as significant, compared to the 4 we’d expect. It’s helpful to take a step back and contemplate our null hypothesis of AR(1) background, and the real possibility that without adjustments, we might be underestimating the lag-1 autocorrelation, hence making the test too lenient. There are alternatives to that in Astrochron, using either the robust method of Mann &amp; Lees (1996) or a less error-prone version by the author Meyers, (2012), called LOWSPEC. You might want to experiment with that. Another factor to play with is the padding factor (padfac). In a nutshell, padding helps increase the spectral resolution at little to no cost (see here for an informal account). You should also try and see what happens when it varies. (note: we used the Astrochron default values here) Most important of all is the choice of null (Vaughan et al, 2011). Past work has shown that the continuum of climate variability is well described by power laws Huybers &amp; Curry (2006), so this should be a far better null against which to test the emergence of spectral peaks. Never fear! Astrochron has an app for that: mtmPL.df &lt;- computeSpectraEns(t,X,method = &#39;mtm&#39;, tbw=tbw, mtm_null = &#39;power_law&#39;) # tbw is the time-bandwidth product, p in the above sig.freqPL &lt;- astrochron::mtmPL(dfe,tbw=tbw,padfac=5,genplot = F,output=2, verbose = F, flow=f.low, fhigh=f.high) mtm.df = mtmPL.df[c(1,2)] names(mtm.df)[1] &lt;- &quot;freq&quot; names(mtm.df)[2] &lt;- &quot;pwr&quot; cl.df &lt;- data.frame(mtmPL.df$freqs,mtmPL.df$power.CL) # extract confidence limits # rename columns to be less silly names(cl.df)[1] &lt;- &quot;freq&quot; #names(cl.df)[2] &lt;- &quot;90% CL&quot; names(cl.df)[2] &lt;- &quot;95% CL&quot; #names(cl.df)[4] &lt;- &quot;99% CL&quot; # plot it p.mtmPL &lt;- plotSpectrum(mtm.df,cl.df,x.lims = prange,x.ticks = pticks, y.lims = c(1e-6,10),color.line=&#39;orange&#39;, color.cl=&#39;white&#39;) + ggtitle(&quot;IODP 846 d18O, Multi-taper method, power-law null&quot;) + xlab(&quot;Period (ky)&quot;) + theme_hc(style = &quot;darkunica&quot;) + theme(axis.ticks.x = element_line(color = &quot;gray&quot;)) # label periodicities of interest p.mtmPL &lt;- periodAnnotate(p.mtmPL, periods = c(19,23,41,100), y.lims = c(1e-6,1)) p.mtmPL &lt;- periodAnnotate(p.mtmPL, periods = 1/sig.freqPL$Frequency,color = &quot;chartreuse&quot;,y.lims = c(1e-6,.1)) show(p.mtmPL) Sure enough, this gets rid of a few cycles, but many remain. A few regions of the spectrum poke above the 95% confidence limit, but one should not be so foolish as to identify every single periodicity in these ranges to be independent. This is related to the multiple hypothesis problem. However, as pointed out by Meyers, (2012), even that misses the point: sedimentary processes (and many processes in other proxy archives) tend to smooth out the signal over the depth axis (hence, over time), making comparisons at neighboring frequencies highly dependent. One solution is to use predictions made by a physical model about the frequency and relative amplitude of astronomical cycles Meyers &amp; Sageman, 2007. However this may not be applicable to all spectral detection problems. We thus refrain from implementing “cookie-cutter” solutions in GeoChronR, to force the user to think deeply about the null hypothesis and the most sensible way to test it. 7.13.3 Weighted Wavelet Z-transform (nuspectral) An interpolation-free alternative to MTM is the Weighted Wavelet Z-transform of Foster, (1996). The idea of spectral analysis is to decompose the signal on an orthogonal basis of sines and cosines. Data gaps cause this basis to lose its orthogonality, creating energy leakage. WWZ mitigates this problem using an inverse approach. Because it is wavelet based, it does not rely on interpolation or detrending. WWZ was adapted by Kirchner &amp; Neal (2013), who employed basis rotations to mitigate the numerical instability that occurs in pathological cases with the original algorithm. A paleoclimate example of its use is given in Zhu et al (2019)). The WWZ method has one adjustable parameter, a decay constant that balances the time resolution and frequency resolution of the wavelet analysis. The smaller this constant is, the sharper the peaks. We wanted to offer GeoChronR users a better alternative to Lom-Scargle, but the WWZ code is relatively complex. We thus chose to incoporate the nuspectral method of Mathias et al (2004) based on a similar idea. However, it is not equivalent to WWZ. The main difference has to do with the approximation of the mother wavelet by a cubic polynomial with compact support. This speeds up computation at the cost of much accuracy. Mathias et al (2004) also describe implementations of complex wavelets in C (a compiled language, faster with loops), but we were not able to obtain any sensible results with those. For now, it is incorporated into GeoChronR to encourage further development. Readers interested in a more robust implementation of WWZ may want to consider pyleoclim, a Python package. This can be called in R via the Reticulate package. 7.14 Time-uncertain spectral analysis Now let’s consider age uncertainties. The GeoChronR approach to quantifying and propagating those uncertainties is to leverage the power of ensembles. Here we will illustrate this with MTM. Let us repeat the previous analysis by looping over the 1,000 age realizations output by the tuning algorithm HMM-Match. That means computing 1000 spectra. We’ve already seen that nuspectral is too slow for an ensemble job, so let’s leave it out. Also, since REDFIT is a tapered version of Lomb-Scargle, and comes with more features, let’s focus here on comparing an REDFIT and MTM in ensemble mode. time = age$values[age.median &lt; 1000,] # age ensemble for ~ last 1 Ma values = temp$values[age.median &lt; 1000] mtm.ens.spec = computeSpectraEns(time,values, max.ens = 1000, method = &#39;mtm&#39;, tbw=tbw, padfac=5, mtm_null = &#39;AR(1)&#39;) This took a few seconds with 1000 ensemble members, and the resulting output is close to 10 Mb. Not an issue for modern computers, but you can see why people weren’t doing this in the 70’s, even if they wanted to. Now, let’s plot the result: ar1.df &lt;- data.frame(mtm.ens.spec$freqs,mtm.ens.spec$power.CL) # rename columns to be less silly names(ar1.df)[1] &lt;- &quot;freq&quot; names(ar1.df)[2] &lt;- &quot;95% CL&quot; p.mtm.ens &lt;- plotSpectraEns(spec.ens = mtm.ens.spec, cl.df = ar1.df, x.lims = c(10,150), y.lims = c(1e-5,1e-1), color.line=&#39;darkorange&#39;, color.low=&#39;darkorange2&#39;, color.high=&#39;coral4&#39;,color.cl = &#39;white&#39;) + ggtitle(&quot;IODP 846 age ensemble, MTM&quot;) + theme_hc(style = &quot;darkunica&quot;) + theme(axis.ticks.x = element_line(color = &quot;gray&quot;)) # label periodicities of interest p.mtm.ens &lt;- periodAnnotate(p.mtm.ens, periods = c(19,23,41,100), y.lims =c(0.01,0.05)) show(p.mtm.ens) To this we can add the periods identified as significant owing to MTM’s harmonic ratio test. geoChronR deals with it by computing at each frequency the fraction of ensemble members that exhibit a significant peak. One simple criterion for gauging the level of support for such peaks given age uncertainties is to pick out those periodicities that are identified as significant more than 50% of the time. per = 1/cl.df$freq sig_per = which(mtm.ens.spec$sig.freq&gt;0.5) p.mtm.ens &lt;- periodAnnotate(p.mtm.ens, periods = per[sig_per],color = &quot;chartreuse&quot;,y.lims = c(1e-5,5e-4)) show(p.mtm.ens) One could of course, impose a more stringent criterion (e.g. 80%, 90%, etc). To do that, specify which(mtm.ens.spec$sig.freq&gt;0.8) or which(mtm.ens.spec$sig.freq&gt;0.9). That relatively close peaks appear significant may be an artifact of neglecting the multiple hypothesis problem (cf Vaughan et al, 2011). So what do we find? Consistent with previous investigations (e.g. Mudelsee et al 2009), the effect of age uncertainties is felt more at high frequencies, with the effect of broadening peaks, or lumping them together. Again, the peaks that rise above the power-law background are the ~100kyr and ~40kyr peaks, which is not surprising. There is significant energy around the precessional peaks, but it is rather spread out, and hard to tie to particular harmonics. No doubt a record with higher-resolution and/or tighter chronology would show sharper precessional peaks. Do we get the same answer in REDFIT? Let’s find out. spec.redfit.ens &lt;- computeSpectraEns(time,values,max.ens = 1000, method = &#39;redfit&#39;) cl.ens.df &lt;- data.frame(&quot;freq&quot; = spec.redfit.ens$freq, &quot;pwr95&quot; = spec.redfit.ens$power.CL) # store confidence limits names(cl.ens.df)[2] &lt;- &quot;95% CL&quot; again, it took a bit of time. let’s plot the result: p.redfit.ens &lt;- plotSpectraEns(spec.ens = spec.redfit.ens, cl.df = cl.ens.df, x.lims = c(10,150), color.line=&#39;darkorange&#39;, color.low=&#39;darkorange2&#39;, color.high=&#39;coral4&#39;, color.cl = &#39;white&#39;) + ggtitle(&quot;IODP 846 age ensemble, redfit&quot;) + theme_hc(style = &quot;darkunica&quot;) + theme(axis.ticks.x = element_line(color = &quot;gray&quot;)) # label periodicities of interest p.redfit.ens &lt;- periodAnnotate(p.redfit.ens, periods = c(19,23,41,100), y.lims =c(0.01,500)) show(p.redfit.ens) The result is quite a bit smoother here, perhaps because of the choice of parameters. 7.15 Conclusion There are several takeaways from this example: geoChronR implements 4 different spectral methods, with different features and assumptions. This allows users to explore whether results obtained with one method are robust to these choices. The methods are enabled for single age models, as well as age ensembles. The choice of null hypothesis is always highly consequential, and should be made with care. Hunting for astronomical (or other) frequencies in paleoclimate timeseries takes extreme care, and cannot be answered in one function call. In this particular example where the data are nearly evenly spaced, and interpolation has a negligible impact, the best choice is MTM for two reasons: (1) because it is optimal, in the sense that it minimizes spectral leakage, and (2) because the implementation used here (derived from astrochron) allows to flexibly apply different tests of significance. Not such luxury is afforded by the other methods at present. There is no one-size-fits-all method that will solve your exact problem, but it might get you started on asking relevant questions. Readers are invited to consider Vaughan et al, 2011 and the work of Stephen Meyers on this topic: Meyers, 2012 Meyers, 2015 Meyers &amp; Malinverno 2018 "]]
